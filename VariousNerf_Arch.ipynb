{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e096cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d234dc4",
   "metadata": {},
   "source": [
    "Instead of concatenating the directional encoding features at the sigma layer we concatenate at the \n",
    "beginning of later mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45fd49bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNerfV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_pos = 10 ## subject to change\n",
    "        self.L_dir = 4 ## subject to change \n",
    "        pos_enc_features = 3+3*2*self.L_pos #tune this parameters \n",
    "        dir_enc_features = 3+3*2*self.L_dir \n",
    "        in_features = pos_enc_features\n",
    "        num_neurons = 256\n",
    "        \n",
    "        in_features = pos_enc_features\n",
    "        self.early_mlp = nn.Sequential(nn.Linear(in_features,256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256,256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256,256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256,256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256,256),\n",
    "                                       nn.ReLU(),\n",
    "                \n",
    "        )\n",
    "        in_features = num_neurons + dir_enc_features+pos_enc_features\n",
    "        self.later_mlp=nn.Sequential(\n",
    "                nn.Linear(in_features,256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256,256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256,256),\n",
    "                nn.ReLU())\n",
    "        self.sigma_layer = nn.Linear(num_neurons, num_neurons+1)\n",
    "        self.pre_final_layer = nn.Sequential(nn.Linear(dir_enc_features+num_neurons+pos_enc_features,num_neurons//2),\n",
    "                                            nn.ReLU())\n",
    "       ### Yeah think about what can be done to this layer actually!!!\n",
    "        \n",
    "        self.final_layer = nn.Sequential(nn.Linear(num_neurons//2,3), nn.Sigmoid())\n",
    "        \n",
    "    def  forward(self, ray_samples, view_dirs):\n",
    "        rays_samples_encoded = [ray_samples]\n",
    "        for l_pos in range(self.L_pos):\n",
    "            rays_samples_encoded.append(torch.sin(2 ** l_pos * torch.pi * ray_samples))\n",
    "            rays_samples_encoded.append(torch.cos(2 ** l_pos * torch.pi * ray_samples))\n",
    "        \n",
    "        rays_samples_encoded = torch.cat(rays_samples_encoded, dim=-1)\n",
    "        \n",
    "        \n",
    "        view_dirs = view_dirs / view_dirs.norm(p=2, dim=-1).unsqueeze(-1)\n",
    "        view_dirs_encoded = [view_dirs]\n",
    "        for l_dir in range(self.L_dir):\n",
    "            view_dirs_encoded.append(torch.sin(2 ** l_dir * torch.pi * view_dirs))\n",
    "            view_dirs_encoded.append(torch.cos(2 ** l_dir * torch.pi * view_dirs))\n",
    "        view_dirs_encoded = torch.cat(view_dirs_encoded,dim=-1)\n",
    "        \n",
    "        outputs = self.early_mlp(rays_samples_encoded)\n",
    "        print('Data types')\n",
    "        print(f'type of view_dirs_encoded:{type(view_dirs_encoded)}')\n",
    "        print(f'type of rays_samples_encoded:{type(rays_samples_encoded)}')\n",
    "        print(f'type of outputs:{type(outputs)}')\n",
    "        outputs = self.later_mlp(torch.cat([outputs, view_dirs_encoded,rays_samples_encoded], dim=-1))\n",
    "        outputs = self.sigma_layer(outputs)\n",
    "        sigma_is = torch.relu(outputs[:,0])\n",
    "        outputs = self.pre_final_layer(torch.cat([view_dirs_encoded, outputs[:, 1:],rays_samples_encoded], dim=-1))\n",
    "        c_is = self.final_layer(outputs)\n",
    "        \n",
    "        return {\"c_is\":c_is,\"sigma_is\":sigma_is}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e1136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nerfmodelv1\n",
    "ds_batch = torch.randn(16834,3)\n",
    "ray_dirs = torch.randn(16834,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e03ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ModelNerfV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a222e33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types\n",
      "type of view_dirs_encoded:<class 'torch.Tensor'>\n",
      "type of rays_samples_encoded:<class 'torch.Tensor'>\n",
      "type of outputs:<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "outputs = model1(ray_dirs,ds_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31a98260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5004, 0.4717, 0.5467],\n",
       "        [0.5252, 0.4875, 0.5379],\n",
       "        [0.5305, 0.4967, 0.5348],\n",
       "        ...,\n",
       "        [0.5074, 0.5076, 0.5117],\n",
       "        [0.4863, 0.5236, 0.5465],\n",
       "        [0.5072, 0.5115, 0.5133]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['c_is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ac0deb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0203, 0.0015, 0.0127,  ..., 0.0146, 0.0071, 0.0224],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['sigma_is']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff5b00",
   "metadata": {},
   "source": [
    "Now we actually explore a skip connection and actually concatenate the positions and directions from the beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec59893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make it more dense just like skip connections in resnet \n",
    "## Essentially at one point we would just add instead of concatenating\n",
    "class VanillaSkipNerfV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_pos = 10 ## subject to change\n",
    "        self.L_dir = 4 ## subject to change \n",
    "        pos_enc_features = 3+3*2*self.L_pos #tune this parameters \n",
    "        dir_enc_features = 3+3*2*self.L_dir \n",
    "        in_features = pos_enc_features + dir_enc_features\n",
    "        num_neurons = 256\n",
    "        \n",
    "        self.early_mlp = nn.Sequential(nn.Linear(in_features,256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256,256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256,256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256,256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256,256),\n",
    "                                       nn.ReLU(),\n",
    "                                      )\n",
    "        in_features = pos_enc_features + num_neurons\n",
    "        \n",
    "        self.later_mlp = nn.sequential(nn.Linear(in_features,256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256,256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(256,256),\n",
    "                                       nn.ReLU(),\n",
    "                                      )\n",
    "        self.sigma_layer = nn.Linear(num_neurons, num_neurons+1)\n",
    "        self.pre_final_layer = nn.Sequential(nn.Linear(dir_enc_features+num_neurons,num_neurons//2),\n",
    "                                            nn.ReLU())\n",
    "        self.final_layer = nn.Sequential(nn.Linear(num_neurons//2,3), nn.Sigmoid())\n",
    "        \n",
    "     ## Here we would just add the vanilla skip   \n",
    "    def forward(self,rays_samples, view_dirs):\n",
    "        \n",
    "        rays_samples_encoded = [rays_samples]\n",
    "        skip1 = \n",
    "        for l_pos in range(self.L_pos):\n",
    "            rays_samples_encoded.append(torch.sin(2**l_pos*torch.pi*rays_samples))\n",
    "            rays_samples_encoded.append(torch.cos(2**l_pos*torch.pi*rays_samples))\n",
    "        \n",
    "        ## Probably after going through few blocks I wanna add it\n",
    "        rays_samples_encoded = torch.cat(rays_samples_encoded, dim = -1)\n",
    "        \n",
    "        ## introduce some skip connections here\n",
    "        #?\n",
    "        rays_samples_encoded = rays_samples_encoded+ skip1 ## remember both now have different dimensions\n",
    "        \n",
    "        \n",
    "        view_dirs = view_dirs/view_dirs.norm(p=2, dim=-1).unsqueeze(-1)\n",
    "        view_dirs_encoded = [view_dirs]\n",
    "        \n",
    "        for l_dir in range(self.L_dir):\n",
    "            view_dirs_encoded.append(torch.sin(2**l_dir*torch.pi*view_dirs))\n",
    "            view_dirs_encoded.append(torch.cos(2**l_dir*torch.pi*view_dirs))\n",
    "            \n",
    "        view_dirs_encoded = torch.cat(view_dirs_encoded, dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_blocks(self, in_channels,intermediate_channels, num_repeat, expansion, is_Bottleneck, stride):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44d4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vanilla skip nerf architecture with 12 layers of MLP \n",
    "\n",
    "class vanilla_skip_nerf_12(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_pos = 10\n",
    "        self.L_dir = 4\n",
    "        pos_enc_features = 3*2*self.L_pos ##Don't add 3 here\n",
    "        dir_enc_features = 3*2*self.L_dir ## Don't add 3 here\n",
    "        in_pos_features1 = pos_enc_features\n",
    "        in_dir_features2 = dir_enc_features\n",
    "        num_neurons = 256\n",
    "        \n",
    "        # In one mlp_block, we would feed the pos\n",
    "        # In the other we would  feed the dir enc\n",
    "        self.mlp_block_1 = nn.Sequential(\n",
    "            nn.Linear(in_pos_features1,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "        self.mlp_block_2 = nn.Sequential(\n",
    "            nn.Linear(in_dir_features2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "        self.mlp_block_3 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "#         self.sigma_layer =\n",
    "#         self.pre_final_layer =\n",
    "#         self.final_layer =\n",
    "    def forward(self, rays_samples,view_dirs):\n",
    "        rays_samples_encoded = [rays_samples]\n",
    "        for l_pos in range(self.L_pos):\n",
    "            rays_samples_encoded.append(torch.sin(2**l_pos*torch.pi*rays_samples))\n",
    "            rays_samples_encoded.append(torch.cos(2**l_pos*torch.pi*rays_samples))\n",
    "        \n",
    "        rays_samples_encoded = torch.cat(rays_samples_encoded, dim=-1) ##(16K *60)\n",
    "        print(f'shape of rays_samples_encoded:{rays_samples_encoded.shape}')\n",
    "        \n",
    "        \n",
    "        view_dirs = view_dirs/view_dirs.norm(p=2, dim=-1).unsqueeze(-1)\n",
    "        view_dirs_encoded = [view_dirs]\n",
    "        for l_dir in range(self.L_dir):\n",
    "            view_dirs_encoded.append(torch.sin(2**l_dir*torch.pi*view_dirs))\n",
    "            view_dirs_encoded.append(torch.cos(2**l_dir*torch.pi*view_dirs))\n",
    "        \n",
    "        view_dirs_encoded = torch.cat(view_dirs_encoded, dim=-1) ##(16*24)\n",
    "        \n",
    "        \n",
    "        print(f'shape of view_dirs_encoded:{view_dirs_encoded.shape}')\n",
    "        \n",
    "        outputs_mlp_block1 = self.mlp_block_1(rays_samples_encoded)\n",
    "        print(f'outputs_mlp_block1:{outputs_mlp_block1.shape}')\n",
    "        outputs_mlp_block2 = self.mlp_block_2(view_dirs_encoded) ## outputs1 + outputs 2\n",
    "        print(f'outputs_mlp_block2:{outputs_mlp_block2.shape}')\n",
    "        # Sum the outputs \n",
    "        outputs_combined = outputs_mlp_block1+outputs_mlp_block2\n",
    "        print(f'outputs_combined:{outputs_combined.shape}')\n",
    "        # Now Concatenate with positional and directional embeddings\n",
    "        outputs_mlp_block3 = self.mlp_block_3(outputs_combined)\n",
    "        print(f'outputs_mlp_block3:{outputs_mlp_block3.shape}')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbca8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_skip_nerf_12_model1 = vanilla_skip_nerf_12()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12323c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of rays_samples_encoded:torch.Size([16834, 63])\n",
      "shape of view_dirs_encoded:torch.Size([16834, 27])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16834x63 and 60x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvanilla_skip_nerf_12_model1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mray_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mds_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_cnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_cnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m, in \u001b[0;36mvanilla_skip_nerf_12.forward\u001b[1;34m(self, rays_samples, view_dirs)\u001b[0m\n\u001b[0;32m     62\u001b[0m view_dirs_encoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(view_dirs_encoded, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m##(16*24)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape of view_dirs_encoded:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview_dirs_encoded\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m outputs_mlp_block1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_block_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrays_samples_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs_mlp_block1:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs_mlp_block1\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     69\u001b[0m outputs_mlp_block2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_block_2(view_dirs_encoded) \u001b[38;5;66;03m## outputs1 + outputs 2\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_cnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_cnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_cnn\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_cnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_cnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_cnn\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16834x63 and 60x256)"
     ]
    }
   ],
   "source": [
    "vanilla_skip_nerf_12_model1(ray_dirs,ds_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189cef9c",
   "metadata": {},
   "source": [
    "So, Now you see why the +3 was given to self.L_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756e372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_skip_nerf_12(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_pos = 10\n",
    "        self.L_dir = 4\n",
    "        pos_enc_features = 3+3*2*self.L_pos \n",
    "        dir_enc_features = 3+3*2*self.L_dir \n",
    "        in_pos_features1 = pos_enc_features\n",
    "        in_dir_features2 = dir_enc_features\n",
    "        num_neurons = 256\n",
    "        \n",
    "        # In one mlp_block, we would feed the pos\n",
    "        # In the other we would  feed the dir enc\n",
    "        self.mlp_block_1 = nn.Sequential(\n",
    "            nn.Linear(in_pos_features1,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "        self.mlp_block_2 = nn.Sequential(\n",
    "            nn.Linear(in_dir_features2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "        self.mlp_block_3 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "#         self.sigma_layer =\n",
    "#         self.pre_final_layer =\n",
    "#         self.final_layer =\n",
    "    def forward(self, rays_samples,view_dirs):\n",
    "        rays_samples_encoded = [rays_samples]\n",
    "        for l_pos in range(self.L_pos):\n",
    "            rays_samples_encoded.append(torch.sin(2**l_pos*torch.pi*rays_samples))\n",
    "            rays_samples_encoded.append(torch.cos(2**l_pos*torch.pi*rays_samples))\n",
    "        \n",
    "        rays_samples_encoded = torch.cat(rays_samples_encoded, dim=-1) ##(16K *60)\n",
    "        print(f'shape of rays_samples_encoded:{rays_samples_encoded.shape}')\n",
    "        \n",
    "        \n",
    "        view_dirs = view_dirs/view_dirs.norm(p=2, dim=-1).unsqueeze(-1)\n",
    "        view_dirs_encoded = [view_dirs]\n",
    "        for l_dir in range(self.L_dir):\n",
    "            view_dirs_encoded.append(torch.sin(2**l_dir*torch.pi*view_dirs))\n",
    "            view_dirs_encoded.append(torch.cos(2**l_dir*torch.pi*view_dirs))\n",
    "        \n",
    "        view_dirs_encoded = torch.cat(view_dirs_encoded, dim=-1) ##(16*24)\n",
    "        \n",
    "        \n",
    "        print(f'shape of view_dirs_encoded:{view_dirs_encoded.shape}')\n",
    "        \n",
    "        outputs_mlp_block1 = self.mlp_block_1(rays_samples_encoded)\n",
    "        print(f'outputs_mlp_block1:{outputs_mlp_block1.shape}')\n",
    "        outputs_mlp_block2 = self.mlp_block_2(view_dirs_encoded) ## outputs1 + outputs 2\n",
    "        print(f'outputs_mlp_block2:{outputs_mlp_block2.shape}')\n",
    "        # Sum the outputs \n",
    "        outputs_combined = outputs_mlp_block1+outputs_mlp_block2\n",
    "        print(f'outputs_combined:{outputs_combined.shape}')\n",
    "        # Now Concatenate with positional and directional embeddings\n",
    "        outputs_mlp_block3 = self.mlp_block_3(outputs_combined)\n",
    "        print(f'outputs_mlp_block3:{outputs_mlp_block3.shape}')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c6cbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_skip_nerf_12_model1 = vanilla_skip_nerf_12()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa30a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of rays_samples_encoded:torch.Size([16834, 63])\n",
      "shape of view_dirs_encoded:torch.Size([16834, 27])\n",
      "outputs_mlp_block1:torch.Size([16834, 256])\n",
      "outputs_mlp_block2:torch.Size([16834, 256])\n",
      "outputs_combined:torch.Size([16834, 256])\n",
      "outputs_mlp_block3:torch.Size([16834, 256])\n"
     ]
    }
   ],
   "source": [
    "vanilla_skip_nerf_12_model1(ray_dirs,ds_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84e36fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this architecture we have added output of a 4 block MLP of the positional and directional encodings and have just concatenated the directional encodings at the third block and have kept everything same\n",
    "class vanilla_skip_nerf_12(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_pos = 10\n",
    "        self.L_dir = 4\n",
    "        pos_enc_features = 3+3*2*self.L_pos \n",
    "        dir_enc_features = 3+3*2*self.L_dir \n",
    "        in_pos_features1 = pos_enc_features\n",
    "        in_dir_features2 = dir_enc_features\n",
    "        num_neurons = 256\n",
    "        \n",
    "        # In one mlp_block, we would feed the pos\n",
    "        # In the other we would  feed the dir enc\n",
    "        self.mlp_block_1 = nn.Sequential(\n",
    "            nn.Linear(in_pos_features1,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "        self.mlp_block_2 = nn.Sequential(\n",
    "            nn.Linear(in_dir_features2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "        in_features_3 = num_neurons+dir_enc_features\n",
    "        self.mlp_block_3 = nn.Sequential(\n",
    "            nn.Linear(in_features_3,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "        self.sigma_layer = nn.Linear(num_neurons, num_neurons+1) ## In next version just change the number of neurons\n",
    "        self.pre_final_layer = nn.Sequential(nn.Linear(num_neurons, num_neurons//2),\n",
    "                                            nn.ReLU())\n",
    "        self.final_layer= nn.Sequential(nn.Linear(num_neurons//2,3,nn.Sigmoid()))\n",
    "        \n",
    "    def forward(self, rays_samples,view_dirs):\n",
    "        rays_samples_encoded = [rays_samples]\n",
    "        for l_pos in range(self.L_pos):\n",
    "            rays_samples_encoded.append(torch.sin(2**l_pos*torch.pi*rays_samples))\n",
    "            rays_samples_encoded.append(torch.cos(2**l_pos*torch.pi*rays_samples))\n",
    "        \n",
    "        rays_samples_encoded = torch.cat(rays_samples_encoded, dim=-1) ##(16K *60)\n",
    "        print(f'shape of rays_samples_encoded:{rays_samples_encoded.shape}')\n",
    "        \n",
    "        \n",
    "        view_dirs = view_dirs/view_dirs.norm(p=2, dim=-1).unsqueeze(-1)\n",
    "        view_dirs_encoded = [view_dirs]\n",
    "        for l_dir in range(self.L_dir):\n",
    "            view_dirs_encoded.append(torch.sin(2**l_dir*torch.pi*view_dirs))\n",
    "            view_dirs_encoded.append(torch.cos(2**l_dir*torch.pi*view_dirs))\n",
    "        \n",
    "        view_dirs_encoded = torch.cat(view_dirs_encoded, dim=-1) ##(16*24)\n",
    "        \n",
    "        \n",
    "        print(f'shape of view_dirs_encoded:{view_dirs_encoded.shape}')\n",
    "        \n",
    "        outputs_mlp_block1 = self.mlp_block_1(rays_samples_encoded)\n",
    "        print(f'outputs_mlp_block1:{outputs_mlp_block1.shape}')\n",
    "        outputs_mlp_block2 = self.mlp_block_2(view_dirs_encoded) ## outputs1 + outputs 2\n",
    "        print(f'outputs_mlp_block2:{outputs_mlp_block2.shape}')\n",
    "        # Sum the outputs \n",
    "        outputs_combined = outputs_mlp_block1+outputs_mlp_block2\n",
    "        print(f'outputs_combined:{outputs_combined.shape}')\n",
    "        output_concat = torch.cat([outputs_combined,view_dirs_encoded],dim=-1) ## We have just concatenated the directional encodings\n",
    "        print(f'shape of output concat: {output_concat.shape}') \n",
    "\n",
    "        # Now Concatenate with positional and directional embeddings\n",
    "        outputs_mlp_block3 = self.mlp_block_3(output_concat)\n",
    "        print(f'outputs_mlp_block3:{outputs_mlp_block3.shape}')\n",
    "        ## At this point we are again concatenating just the directional stuff\n",
    "        outputs = self.sigma_layer(outputs_mlp_block3)\n",
    "        print(f'After passing sigma layer: {outputs_mlp_block3.shape}')\n",
    "        sigma_is = torch.relu(outputs[:,0])\n",
    "        print(f'sigma is: {sigma_is.shape}')\n",
    "        outputs1 = self.pre_final_layer(outputs[:,1:])\n",
    "        print(f'shape of output after passing pre_final_layer: {outputs1.shape}')\n",
    "        c_is = self.final_layer(outputs1)\n",
    "        print(f'shape of c_is:{c_is.shape}')\n",
    "        return {\"c_is\":c_is, \"sigma_is\": sigma_is}\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "029eab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_skip_nerf_12_model2 = vanilla_skip_nerf_12()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "825f1ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of rays_samples_encoded:torch.Size([16834, 63])\n",
      "shape of view_dirs_encoded:torch.Size([16834, 27])\n",
      "outputs_mlp_block1:torch.Size([16834, 256])\n",
      "outputs_mlp_block2:torch.Size([16834, 256])\n",
      "outputs_combined:torch.Size([16834, 256])\n",
      "shape of output concat: torch.Size([16834, 283])\n",
      "outputs_mlp_block3:torch.Size([16834, 256])\n",
      "After passing sigma layer: torch.Size([16834, 256])\n",
      "sigma is: torch.Size([16834])\n",
      "shape of output after passing pre_final_layer: torch.Size([16834, 128])\n",
      "shape of c_is:torch.Size([16834, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c_is': tensor([[-0.0149,  0.0030, -0.0112],\n",
       "         [-0.0154,  0.0034, -0.0118],\n",
       "         [-0.0152,  0.0036, -0.0111],\n",
       "         ...,\n",
       "         [-0.0147,  0.0037, -0.0114],\n",
       "         [-0.0146,  0.0037, -0.0111],\n",
       "         [-0.0154,  0.0038, -0.0124]], grad_fn=<AddmmBackward0>),\n",
       " 'sigma_is': tensor([0., 0., 0.,  ..., 0., 0., 0.], grad_fn=<ReluBackward0>)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_skip_nerf_12_model2(ray_dirs,ds_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60d30c",
   "metadata": {},
   "source": [
    "Looks Like this nerf architecture do not work well, it starts with a huge lose compared to the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcb906e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_skip_nerf_12_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_pos = 10\n",
    "        self.L_dir = 4\n",
    "        pos_enc_features = 3+3*2*self.L_pos \n",
    "        dir_enc_features = 3+3*2*self.L_dir \n",
    "        in_pos_features1 = pos_enc_features\n",
    "        \n",
    "        num_neurons = 256\n",
    "        in_dir_features2 = num_neurons+dir_enc_features\n",
    "       \n",
    "        self.mlp_block_1 = nn.Sequential(\n",
    "            nn.Linear(in_pos_features1,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "        self.mlp_block_2 = nn.Sequential(\n",
    "            nn.Linear(in_dir_features2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "        in_features_3 = num_neurons\n",
    "        self.mlp_block_3 = nn.Sequential(\n",
    "            nn.Linear(in_features_3,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU())\n",
    "        self.sigma_layer = nn.Linear(num_neurons, num_neurons+1) ## In next version just change the number of neurons\n",
    "        self.pre_final_layer = nn.Sequential(nn.Linear(num_neurons, num_neurons//2),\n",
    "                                            nn.ReLU())\n",
    "        self.final_layer= nn.Sequential(nn.Linear(num_neurons//2,3,nn.Sigmoid()))\n",
    "        \n",
    "    def forward(self, rays_samples,view_dirs):\n",
    "        rays_samples_encoded = [rays_samples]\n",
    "        for l_pos in range(self.L_pos):\n",
    "            rays_samples_encoded.append(torch.sin(2**l_pos*torch.pi*rays_samples))\n",
    "            rays_samples_encoded.append(torch.cos(2**l_pos*torch.pi*rays_samples))\n",
    "        \n",
    "        rays_samples_encoded = torch.cat(rays_samples_encoded, dim=-1) ##(16K *60)\n",
    "        print(f'shape of rays_samples_encoded:{rays_samples_encoded.shape}')\n",
    "        \n",
    "        \n",
    "        view_dirs = view_dirs/view_dirs.norm(p=2, dim=-1).unsqueeze(-1)\n",
    "        view_dirs_encoded = [view_dirs]\n",
    "        for l_dir in range(self.L_dir):\n",
    "            view_dirs_encoded.append(torch.sin(2**l_dir*torch.pi*view_dirs))\n",
    "            view_dirs_encoded.append(torch.cos(2**l_dir*torch.pi*view_dirs))\n",
    "        \n",
    "        view_dirs_encoded = torch.cat(view_dirs_encoded, dim=-1) ##(16*24)\n",
    "        \n",
    "        \n",
    "        print(f'shape of view_dirs_encoded:{view_dirs_encoded.shape}')\n",
    "        \n",
    "        outputs_mlp_block1 = self.mlp_block_1(rays_samples_encoded)\n",
    "        print(f'outputs_mlp_block1:{outputs_mlp_block1.shape}')\n",
    "        outputs_mlp_block2 = self.mlp_block_2(torch.cat([view_dirs_encoded,outputs_mlp_block1], dim=-1)) ##concatenating directional inputs to outputs of previous MLP\n",
    "        print(f'outputs_mlp_block2:{outputs_mlp_block2.shape}')\n",
    "        # Sum the outputs \n",
    "        outputs_combined = outputs_mlp_block1+outputs_mlp_block2 ## Here goes the skip connections \n",
    "        print(f'outputs_combined:{outputs_combined.shape}')\n",
    "#         output_concat = torch.cat([outputs_combined,view_dirs_encoded],dim=-1) ## We have just concatenated the directional encodings\n",
    "#         print(f'shape of output concat: {output_concat.shape}') \n",
    "\n",
    "        # Now Concatenate with positional and directional embeddings\n",
    "        outputs_mlp_block3 = self.mlp_block_3(outputs_combined)\n",
    "        print(f'outputs_mlp_block3:{outputs_mlp_block3.shape}')\n",
    "        ## At this point we are again concatenating just the directional stuff\n",
    "        outputs = self.sigma_layer(outputs_mlp_block3)\n",
    "        print(f'After passing sigma layer: {outputs_mlp_block3.shape}')\n",
    "        sigma_is = torch.relu(outputs[:,0])\n",
    "        print(f'sigma is: {sigma_is.shape}')\n",
    "        outputs1 = self.pre_final_layer(outputs[:,1:])\n",
    "        print(f'shape of output after passing pre_final_layer: {outputs1.shape}')\n",
    "        c_is = self.final_layer(outputs1)\n",
    "        print(f'shape of c_is:{c_is.shape}')\n",
    "        return {\"c_is\":c_is, \"sigma_is\": sigma_is}\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e86f21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_skip_nerf_12_model3 = vanilla_skip_nerf_12_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28478835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of rays_samples_encoded:torch.Size([16834, 63])\n",
      "shape of view_dirs_encoded:torch.Size([16834, 27])\n",
      "outputs_mlp_block1:torch.Size([16834, 256])\n",
      "outputs_mlp_block2:torch.Size([16834, 256])\n",
      "outputs_combined:torch.Size([16834, 256])\n",
      "outputs_mlp_block3:torch.Size([16834, 256])\n",
      "After passing sigma layer: torch.Size([16834, 256])\n",
      "sigma is: torch.Size([16834])\n",
      "shape of output after passing pre_final_layer: torch.Size([16834, 128])\n",
      "shape of c_is:torch.Size([16834, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'c_is': tensor([[-0.0538,  0.0169,  0.0172],\n",
       "         [-0.0538,  0.0167,  0.0172],\n",
       "         [-0.0538,  0.0167,  0.0171],\n",
       "         ...,\n",
       "         [-0.0538,  0.0167,  0.0172],\n",
       "         [-0.0539,  0.0166,  0.0172],\n",
       "         [-0.0537,  0.0166,  0.0172]], grad_fn=<AddmmBackward0>),\n",
       " 'sigma_is': tensor([0., 0., 0.,  ..., 0., 0., 0.], grad_fn=<ReluBackward0>)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_skip_nerf_12_model3(ray_dirs,ds_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e47108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
